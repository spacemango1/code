{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d2b8455",
   "metadata": {},
   "source": [
    "*Adapted from Make Your Own Neural Network by Tariq Rashid: https://github.com/makeyourownneuralnetwork/makeyourownneuralnetwork/blob/master/part2_neural_network.ipynb*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b572b646",
   "metadata": {},
   "source": [
    "# A neural network from scratch\n",
    "\n",
    "In this notebook we will code a basic neural network from scratch, do forward passes with it, and train it with backpropogation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219c5081",
   "metadata": {},
   "source": [
    "## Import required libraries\n",
    "\n",
    "For this example you will need `keras`. You can use either standalone installation of Keras in your Anaconda environment or use `tf.keras` if you have Tensorflow install instead. If you have neither, run `pip install tensorflow` in your Anaconda environemnt then modify `import keras` to `import tf.keras as keras` in the cell below before running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4243248f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import random\n",
    "import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbef9eb7",
   "metadata": {},
   "source": [
    "## Neural Network class\n",
    "\n",
    "Here we have a 3-layer neural network: the input layer, 1 hidden layer, and the output layer. Each layer can have a defined amounts of neurons/nodes. The learning rate is constant and defined on object instance creation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a4a822",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, input_nodes, hidden_nodes, output_nodes, learning_rate):\n",
    "        # set number of nodes in each input, hidden, output layer\n",
    "        self.inodes = input_nodes \n",
    "        self.hnodes = hidden_nodes \n",
    "        self.onodes = output_nodes\n",
    "        \n",
    "        # link weight matrices, wih and who\n",
    "        # weights inside the arrays are w_i_j, where link is from node i to node j in the next layer\n",
    "        # w11 w21\n",
    "        # w12 w22 etc \n",
    "        self.wih = np.random.normal(0.0, pow(self.inodes, -0.5), (self.hnodes, self.inodes))\n",
    "        self.who = np.random.normal(0.0, pow(self.hnodes, -0.5), (self.onodes, self.hnodes))\n",
    "        \n",
    "        # learning rate\n",
    "        self.lr = learning_rate\n",
    "        \n",
    "        # activation function is the sigmoid function\n",
    "        self.activation_function = lambda x: 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    def train(self, inputs_list, targets_list):\n",
    "        # convert inputs list to 2d array\n",
    "        inputs = np.array(inputs_list, ndmin=2).T\n",
    "        targets = np.array(targets_list, ndmin=2).T\n",
    "        \n",
    "        # calculate signals into hidden layer\n",
    "        hidden_inputs = np.dot(self.wih, inputs)\n",
    "        # calculate the signals emerging from hidden layer\n",
    "        hidden_outputs = self.activation_function(hidden_inputs)\n",
    "        \n",
    "        # calculate signals into final output layer\n",
    "        final_inputs = np.dot(self.who, hidden_outputs)\n",
    "        # calculate the signals emerging from final output layer\n",
    "        final_outputs = self.activation_function(final_inputs)\n",
    "        \n",
    "        # output layer error is the (target - actual)\n",
    "        output_errors = targets - final_outputs\n",
    "        # hidden layer error is the output_errors, split by weights, recombined at hidden nodes\n",
    "        hidden_errors = np.dot(self.who.T, output_errors) \n",
    "        \n",
    "        # update the weights for the links between the hidden and output layers\n",
    "        self.who += self.lr * np.dot((output_errors * final_outputs * (1.0 - final_outputs)), np.transpose(hidden_outputs))\n",
    "        \n",
    "        # update the weights for the links between the input and hidden layers\n",
    "        self.wih += self.lr * np.dot((hidden_errors * hidden_outputs * (1.0 - hidden_outputs)), np.transpose(inputs))\n",
    "    \n",
    "    def query(self, inputs_list):\n",
    "        # convert inputs list to 2d array\n",
    "        inputs = np.array(inputs_list, ndmin=2).T\n",
    "        \n",
    "        # calculate signals into hidden layer\n",
    "        hidden_inputs = np.dot(self.wih, inputs)\n",
    "        # calculate the signals emerging from hidden layer\n",
    "        hidden_outputs = self.activation_function(hidden_inputs)\n",
    "        \n",
    "        # calculate signals into final output layer\n",
    "        final_inputs = np.dot(self.who, hidden_outputs)\n",
    "        # calculate the signals emerging from final output layer\n",
    "        final_outputs = self.activation_function(final_inputs)\n",
    "        \n",
    "        return final_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d39ec24",
   "metadata": {},
   "source": [
    "## Define the neural network's shape\n",
    "\n",
    "Here we create a neural network with 784 inputs (28x28), 200 hidden nodes in the hidden layer, and 10 output nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2409a757",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of input, hidden and output nodes\n",
    "input_nodes = 784\n",
    "hidden_nodes = 200\n",
    "output_nodes = 10\n",
    "\n",
    "# learning rate is 0.3\n",
    "learning_rate = 0.3\n",
    "\n",
    "# create instance of neural network\n",
    "n = NeuralNetwork(input_nodes,hidden_nodes,output_nodes, learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1dee36e",
   "metadata": {},
   "source": [
    "## Load data\n",
    "\n",
    "The data we are going to be using is the \"Hello, World!\" of machine learning: the [MNIST dataset](http://yann.lecun.com/exdb/mnist/). It is a simple handwritten digit dataset with that is just the right size for simple neural networks like ours and some others. It is a series of 28x28 grayscale images. \n",
    "\n",
    "We are going to load in the dataset from Keras directly as it is one of the default dataset included with Keras and already in Numpy format so we can just do some minimal processing on it. The data is split up into two sets: the training set contains 60,000 images and the test set contains 10,000 images. Each image has a corresponding label indicating which digit from 0-9 it is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db49f5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the data, split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c9c975a",
   "metadata": {},
   "source": [
    "Here we have a look at a sample of the MNIST data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e326e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "for i in range(9):\n",
    "    ax = plt.subplot(3, 3, i + 1)\n",
    "    plt.imshow(x_train[i], cmap='gray')\n",
    "    plt.title(y_train[i])\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932fb4a3",
   "metadata": {},
   "source": [
    "As the data loaded in has a shape of `(28, 28)` we need to flatten it out before we can pass it into our neural network (as our network only take 1D inputs). In addition the data loaded in is in the range of 0-255, we will need to convert it to 0.0-1.0, this is important as we need the number type of the data to match the type of the weights and floating point numbers gives us the precision we need; 0.0-1.0 range inputs means that our weights can stay small as well, paired with a low learning rate, the network can (ironically) learn faster.\n",
    "\n",
    "As for the output array, we are converting them into binary class matrices or simply the probability of the image being represented by its index position. This essentially means the individual values in the vector will only be 0 or 1 with the 1 value being at the position where it represent our class in this classification problem. Try printing the values of `y_train_flat` to see what it looks like. Here we use the Keras function `keras.utils.to_categorical()` to help us do this conversion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b407a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the data to (60000, 784) and convert values to the range of 0-1\n",
    "x_train_flat = x_train.reshape(60000, 784).astype(\"float32\") / 255\n",
    "x_test_flat = x_test.reshape(10000, 784).astype(\"float32\") / 255\n",
    "print(\"Input shape:\", x_train_flat.shape)\n",
    "\n",
    "# Convert class vectors to binary class matrices\n",
    "num_classes = 10\n",
    "y_train_flat = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test_flat = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "print(\"Output shape:\", y_train_flat.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a10cc73",
   "metadata": {},
   "source": [
    "## Training the neural network\n",
    "\n",
    "Now we can train the network. Neural networks are usually train over epochs, one epoch means one pass over the whole training dataset. You can increate the epoch count below but note that it will mean training will take longer with more epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4ef504",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Epochs is the number of times the training data set is used for training\n",
    "epochs = 1\n",
    "\n",
    "for e in range(epochs):\n",
    "    print(\"Starting epoch\", e)\n",
    "    \n",
    "    # Passing each input data entry and its corresponding expected output to train the nn\n",
    "    for i, input_data in enumerate(x_train_flat):\n",
    "        n.train(input_data, y_train_flat[i])\n",
    "        print(\"Training %d/60000\" % (i+1), end=\"\\r\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99fffcfa",
   "metadata": {},
   "source": [
    "## Assessing the result\n",
    "\n",
    "After training, we can assess how well the neural network is doing. Here we will use the test dataset instead of the training dataset to make sure that the neural network actually learnt what we need it to learn and not found some obscure quirk that the training dataset have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a27b0a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Pick a random index from the test data set\n",
    "random_i = random.randint(0, len(x_test))\n",
    "\n",
    "# Plot the image that we just picked\n",
    "plt.clf()\n",
    "ax = plt.subplot(3, 3, 1)\n",
    "plt.imshow(x_test[random_i], cmap='gray')\n",
    "plt.title(y_test[random_i])\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "# Make a prediction\n",
    "print(\"Prediction:\", np.argmax(n.query(x_test_flat[random_i]).reshape(10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00e871d",
   "metadata": {},
   "source": [
    "We can also pass the whole test dataset and get a accuracy count from the neural network. You can rerun the cell with the training step and come back to this cell to see how much will the accuracy increase with each epoch of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a0d30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_count = 0\n",
    "\n",
    "for i, test_input in enumerate(x_test_flat):\n",
    "    prediction = np.argmax(n.query(test_input).reshape(10))\n",
    "    actual = y_test[i]\n",
    "    \n",
    "    if prediction == actual:\n",
    "        correct_count += 1\n",
    "        \n",
    "accuracy = correct_count / len(x_test)\n",
    "print(\"Accuracy:\", format(accuracy, \".2%\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
