{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Preprocessing\n",
    "\n",
    "Almost always when working with an image dataset, you will have to do some preprocessing and clean up before you can use them to train a model. You will usually also have to apply some of the same steps to any input image for inference as well. In this notebook we will look at two techniques of achieving this and see how well they work for a custom collected dataset.\n",
    "\n",
    "The example below is written given that you are scraping image data yourself. You are encourgage to scrape real data, eg. from wikicommons, instead of using preexisting dataset as those are likely to have been cleaned up before they were published, making some of our steps below pointless."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3245,
     "status": "ok",
     "timestamp": 1637941455121,
     "user": {
      "displayName": "Kenneth Lim",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhCkUBofgtGMtHQq5jpL4qG9rg72rxhKBH01sfp=s64",
      "userId": "16639675290373456477"
     },
     "user_tz": 0
    },
    "id": "z8XheNLDtEu3"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SjaMiUQ561ke"
   },
   "source": [
    "# Manual approach\n",
    "First let's look at a manual approach to preprocessing our image data using OpenCV and Numpy.\n",
    "\n",
    "## Load in data (one-off)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 286
    },
    "executionInfo": {
     "elapsed": 2423,
     "status": "ok",
     "timestamp": 1637338749491,
     "user": {
      "displayName": "Kenneth Lim",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhCkUBofgtGMtHQq5jpL4qG9rg72rxhKBH01sfp=s64",
      "userId": "16639675290373456477"
     },
     "user_tz": 0
    },
    "id": "V8k5fSGbtjz6",
    "outputId": "0162193c-adf0-4a64-ea52-f9d4ff7a72f9"
   },
   "outputs": [],
   "source": [
    "original = cv2.imread(\"../data/Benthall_Hall_A.jpg\", cv2.IMREAD_COLOR)\n",
    "\n",
    "rgb_img = cv2.cvtColor(original, cv2.COLOR_BGR2RGB)\n",
    "plt.imshow(rgb_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J50q2Ybo9YsJ"
   },
   "source": [
    "## Resize the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 286
    },
    "executionInfo": {
     "elapsed": 985,
     "status": "ok",
     "timestamp": 1637338822730,
     "user": {
      "displayName": "Kenneth Lim",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhCkUBofgtGMtHQq5jpL4qG9rg72rxhKBH01sfp=s64",
      "userId": "16639675290373456477"
     },
     "user_tz": 0
    },
    "id": "fRoEh8ZRw64v",
    "outputId": "4aeece65-ce94-4c12-cfa0-4eb471814d7f"
   },
   "outputs": [],
   "source": [
    "rgb_img = cv2.resize(rgb_img, (256, 256))\n",
    "plt.imshow(rgb_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WMVxTeA79n8C"
   },
   "source": [
    "## Flip the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 286
    },
    "executionInfo": {
     "elapsed": 887,
     "status": "ok",
     "timestamp": 1637338874861,
     "user": {
      "displayName": "Kenneth Lim",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhCkUBofgtGMtHQq5jpL4qG9rg72rxhKBH01sfp=s64",
      "userId": "16639675290373456477"
     },
     "user_tz": 0
    },
    "id": "Kk1D2dM14YnP",
    "outputId": "88dfc629-219e-4f27-f6d6-7cfdc355b3ac"
   },
   "outputs": [],
   "source": [
    "flipped_img = cv2.flip(rgb_img, 1)\n",
    "plt.imshow(flipped_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RhGAU7417RJ3"
   },
   "source": [
    "## Rotate the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "trHOoAt46Qog"
   },
   "outputs": [],
   "source": [
    "# https://www.pyimagesearch.com/2017/01/02/rotate-images-correctly-with-opencv-and-python/\n",
    "# rotated_img = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MuAsf5nO9sAw"
   },
   "source": [
    "## Traverse a directory for all images\n",
    "\n",
    "At this point we know how to process a single image, so what we can do now is to apply the same processing to all the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 515
    },
    "executionInfo": {
     "elapsed": 1151,
     "status": "ok",
     "timestamp": 1637339167923,
     "user": {
      "displayName": "Kenneth Lim",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhCkUBofgtGMtHQq5jpL4qG9rg72rxhKBH01sfp=s64",
      "userId": "16639675290373456477"
     },
     "user_tz": 0
    },
    "id": "9PbbPa7j0Kf9",
    "outputId": "4fbc28ad-0939-4baa-8367-cd3f39bf8c33"
   },
   "outputs": [],
   "source": [
    "for root, dirs, files in os.walk(\"../data/wikicommons\"):\n",
    "  path = root.split(os.sep)\n",
    "\n",
    "  for file in files:\n",
    "    _, extension = os.path.splitext(file)\n",
    "\n",
    "    if extension == \".jpg\":\n",
    "      fullpath = \"/\".join(path) + \"/\" + file\n",
    "#       print(fullpath)\n",
    "\n",
    "      current = cv2.imread(fullpath, cv2.IMREAD_COLOR)\n",
    "      converted = cv2.cvtColor(current, cv2.COLOR_BGR2RGB)\n",
    "      converted = cv2.resize(converted, (256, 256))\n",
    "      plt.imshow(converted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4JegxtOLD3g2"
   },
   "source": [
    "# Keras based image preprocessing\n",
    "\n",
    "As most of the above operations are very common when dealing with image based ML. Keras has conveniently wrapped many of these functionalities into simple APIs that we can call. We shall do that below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BKS3cv-JLKK9"
   },
   "source": [
    "## Filter out invalid images\n",
    "\n",
    "It is often the case, especially when your dataset is scrapped automatically from the internet, that some of the data need to be cleaned up. It could be invalid Unicode characters, malformed data structure, or in this case, corrupted images. We need to filter out these images as they couldn't be processed by our model.\n",
    "\n",
    "There are a few options on how to do this. If you are following the image processing steps above, you may not need to do this as the data is already in numerical form. If you are using a larger dataset and using tf.dataset class as we will be doing below, we can either fix those corrupted images or just remove them from the dataset.\n",
    "\n",
    "We will remove these below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1134,
     "status": "ok",
     "timestamp": 1637873704704,
     "user": {
      "displayName": "Kenneth Lim",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhCkUBofgtGMtHQq5jpL4qG9rg72rxhKBH01sfp=s64",
      "userId": "16639675290373456477"
     },
     "user_tz": 0
    },
    "id": "tFMXK-kr0ujA",
    "outputId": "d5b86a13-955e-4f33-c3fa-5d85c174fb6a"
   },
   "outputs": [],
   "source": [
    "num_skipped = 0\n",
    "for root, dirs, files in os.walk(\"../data/wikicommons\"):\n",
    "  path = root.split(os.sep)\n",
    "\n",
    "  for file in files:\n",
    "    _, extension = os.path.splitext(file)\n",
    "\n",
    "    if extension == \".jpg\":\n",
    "      filepath = root + \"/\" + file\n",
    "\n",
    "      try:\n",
    "        fobj = open(filepath, \"rb\")\n",
    "        is_jfif = tf.compat.as_bytes(\"JFIF\") in fobj.peek(10)\n",
    "      finally:\n",
    "        fobj.close()\n",
    "\n",
    "      if not is_jfif:\n",
    "        num_skipped += 1\n",
    "        # Delete corrupted image\n",
    "        os.remove(filepath)\n",
    "\n",
    "print(\"Deleted %d images\" % num_skipped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will load our images directly using Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 271
    },
    "executionInfo": {
     "elapsed": 573,
     "status": "error",
     "timestamp": 1637941566796,
     "user": {
      "displayName": "Kenneth Lim",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhCkUBofgtGMtHQq5jpL4qG9rg72rxhKBH01sfp=s64",
      "userId": "16639675290373456477"
     },
     "user_tz": 0
    },
    "id": "_7QhOpQdD9Wo",
    "outputId": "66f44174-a917-4cf6-e056-216ca702d3eb"
   },
   "outputs": [],
   "source": [
    "# Change this to match your data\n",
    "images_path = \"../data/wikicommons\"\n",
    "training_dataset = keras.preprocessing.image_dataset_from_directory(\n",
    "  images_path,\n",
    "  labels='inferred',\n",
    "  label_mode='categorical',\n",
    "  color_mode=\"rgb\",\n",
    "  batch_size=32,\n",
    "  image_size=(150, 150),\n",
    "  subset=\"training\",\n",
    "  validation_split=0.1,\n",
    "  seed=1\n",
    ")\n",
    "\n",
    "test_dataset = keras.preprocessing.image_dataset_from_directory(\n",
    "  images_path,\n",
    "  labels='inferred',\n",
    "  label_mode='categorical',\n",
    "  color_mode=\"rgb\",\n",
    "  batch_size=32,\n",
    "  image_size=(150, 150),\n",
    "  subset=\"validation\",\n",
    "  validation_split=0.1,\n",
    "  seed=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sq5_wMtWOAQg"
   },
   "source": [
    "## Defining our model\n",
    "\n",
    "We can define our own model using Keras' layers API or we can use one of the prebuilt one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1282,
     "status": "ok",
     "timestamp": 1637938247142,
     "user": {
      "displayName": "Kenneth Lim",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhCkUBofgtGMtHQq5jpL4qG9rg72rxhKBH01sfp=s64",
      "userId": "16639675290373456477"
     },
     "user_tz": 0
    },
    "id": "yZ1ZS6ABn-1D",
    "outputId": "36363886-b5de-46fd-b9da-a05937a57c0b"
   },
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "    keras.Input(shape=(150, 150, 3)),\n",
    "#     keras.layers.RandomRotation(0.1),\n",
    "    keras.layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n",
    "    keras.layers.MaxPooling2D(pool_size=(4, 4)),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dropout(0.5),\n",
    "    keras.layers.Dense(2, activation=\"sigmoid\")\n",
    "])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# model = keras.applications.Xception(weights=None, input_shape=(256, 256, 3), classes=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WJLQXb66OO_4"
   },
   "source": [
    "## Compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 516,
     "status": "ok",
     "timestamp": 1637938250828,
     "user": {
      "displayName": "Kenneth Lim",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhCkUBofgtGMtHQq5jpL4qG9rg72rxhKBH01sfp=s64",
      "userId": "16639675290373456477"
     },
     "user_tz": 0
    },
    "id": "ObnXbMCep1ny"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qz908_PxOUCw"
   },
   "source": [
    "## Train the model\n",
    "\n",
    "Training will probably take awhile. Keep an eye on the \"accuracy\" and \"val_accuracy\" metrics here and see how they evolve over time. What insight does it give you?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 400
    },
    "executionInfo": {
     "elapsed": 1095231,
     "status": "error",
     "timestamp": 1637939348200,
     "user": {
      "displayName": "Kenneth Lim",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhCkUBofgtGMtHQq5jpL4qG9rg72rxhKBH01sfp=s64",
      "userId": "16639675290373456477"
     },
     "user_tz": 0
    },
    "id": "9ZRsDQNLpVDj",
    "outputId": "2f32b150-2e1f-4f34-bd68-bf37fed3e131"
   },
   "outputs": [],
   "source": [
    "model.fit(\n",
    "  training_dataset,\n",
    "  validation_data=test_dataset,\n",
    "  epochs=20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the model\n",
    "\n",
    "After training our model, we can save them out to file so that we can load them in later to continue training with new data or we can load them into a different program and use it to classify images as we've trained them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"./model/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can load in a saved model using the `load_model` function provided and it will load the whole model structure as well as the trained weights as if we were continuing from when we last run `model.save()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = keras.models.load_model(\"./model/\")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also do checkpointing while we are training, so we can stop and resume training from the last checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=\"./checkpoint/\",\n",
    "    save_weights_only=True,\n",
    "    monitor='val_accuracy',\n",
    "    mode='max',\n",
    "    save_best_only=True\n",
    ")\n",
    "\n",
    "# model.load_weights(\"./checkpoint/\")\n",
    "\n",
    "model.fit(\n",
    "    training_dataset,\n",
    "    validation_data=test_dataset,\n",
    "    epochs=20,\n",
    "    callbacks=[model_checkpoint_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = keras.preprocessing.image.load_img(\"../data/wikicommons/Forest/forest.16.jpg\", target_size=(150, 150))\n",
    "input_arr = keras.preprocessing.image.img_to_array(image)\n",
    "input_arr = np.array([input_arr])  # Convert single image to a batch.\n",
    "predictions = model.predict(input_arr)\n",
    "predictions"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyN8KkacIFGVE1ZH+kM3Bjts",
   "collapsed_sections": [],
   "mount_file_id": "1qVrDN7hpfrfBUK5roy5TJWcErWfghtoH",
   "name": "Image Processing.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
